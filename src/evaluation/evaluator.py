from typing import List, Dict, Any
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_recall, context_precision
from ragas.llms import LangchainLLM
from langchain_google_genai import ChatGoogleGenerativeAI
from config.settings import settings
import pandas as pd

class Evaluator:
    def __init__(self):
        # Ragas requires an LLM for evaluation itself
        self.eval_llm = ChatGoogleGenerativeAI(
            model=settings.RAGAS_EVAL_LLM,
            temperature=0.0,
            convert_system_message_to_human=True # Important for Ragas with Gemini
        )
        self.ragas_llm = LangchainLLM(llm=self.eval_llm) # Wrap for Ragas

    def evaluate_rag_system(self,
                            questions: List[str],
                            ground_truths: List[str],
                            retrieved_contexts: List[List[str]],
                            generated_answers: List[str]) -> pd.DataFrame:
        """
        Evaluates the RAG system using Ragas metrics.

        Args:
            questions: List of user questions.
            ground_truths: List of ground truth answers for each question.
            retrieved_contexts: List of lists, where each inner list contains retrieved context strings for a question.
            generated_answers: List of answers generated by the RAG system.

        Returns:
            A pandas DataFrame with evaluation results.
        """
        # Create a Ragas Dataset
        data = {
            "question": questions,
            "answer": generated_answers,
            "contexts": retrieved_contexts,
            "ground_truth": ground_truths
        }
        dataset = Dataset.from_dict(data)

        # Define metrics to evaluate
        metrics = [
            faithfulness,
            answer_relevancy,
            context_recall,
            context_precision,
            # Add other metrics if desired, e.g., semantic_similarity
        ]

        # Configure Ragas with your LLM
        for metric in metrics:
            metric.__setattr__("llm", self.ragas_llm)
            metric.__setattr__("embeddings", self.eval_llm) # For metrics that use embeddings

        print("Starting Ragas evaluation... This might take some time.")
        score = evaluate(dataset, metrics=metrics)
        return score.to_pandas()

# Example usage (can be called from a separate script or notebook)
# if __name__ == "__main__":
#     evaluator = Evaluator()
#
#     # Dummy data for demonstration
#     questions = ["What is the capital of France?", "Who painted the Mona Lisa?"]
#     ground_truths = ["The capital of France is Paris.", "Leonardo da Vinci painted the Mona Lisa."]
#     retrieved_contexts = [
#         ["Paris is the capital and most populous city of France."],
#         ["The Mona Lisa is a half-length portrait painting by Italian artist Leonardo da Vinci."]
#     ]
#     generated_answers = [
#         "Paris is the capital of France.",
#         "Leonardo da Vinci painted the Mona Lisa."
#     ]
#
#     results_df = evaluator.evaluate_rag_system(questions, ground_truths, retrieved_contexts, generated_answers)
#     print(results_df)